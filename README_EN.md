<p align="left">
    English</a>&nbsp ï½œ &nbsp<a href="README.md">ä¸­æ–‡</a>
</p>
<br>

<div align="center">
<h1>
LingoWhale-8B
</h1>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/deeplang-ai/LingoWhale-8B" target="_blank">Hugging Face</a> â€¢ ğŸ¤– <a href="https://www.modelscope.cn/models/DeepLang/LingoWhale-8B" target="_blank">ModelScope</a> â€¢ â›µ <a href="https://wisemodel.cn/models/%E5%8C%97%E4%BA%AC%E6%B7%B1%E8%A8%80%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E8%B4%A3%E4%BB%BB%E5%85%AC%E5%8F%B8/LingoWhale-8B/" target="_blank">Wisemodel</a>
</p>

<div align="center">
<strong>
LingoWhale-8B model open-sourced by DeepLangAI in collaboration with THUNLP Lab ğŸ‰
</strong>
</div>

# Table of Contents

- [Introduction](#introduction)
- [Evaluation](#evaluation)
- [Generated Examples](#generated-examples)
- [Deployment and Inference](#deployment-and-inference)
- [Fine-tuning](#fine-tuning)
- [Open Source License](#open-source-license)

# Introduction

LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.

Along with open-sourcing the model weights, we also provide a Huggingface inference interface and parameter efficient fine-tuning examples like LoRA, making it easier for developers to use the LingoWhale-8B model.

Due to the scale of model parameters, intrinsic issues of large language models like hallucination and relatively weak mathematical computation capabilities persist in LingoWhale-8B. Please understand these issues and evaluate the possible risks before using the model. Future versions of the LingoWhale model will focus on optimizing these areas.

# Evaluation

We tested on the following public evaluation benchmarks:

- [C-Eval](https://arxiv.org/abs/2305.08322) is a Chinese foundation model evaluation benchmark consisting of 13,948 multiple-choice questions, covering 52 different subjects and four difficulty levels. It aims to assess the capability of Chinese language models. We used the dev set of this dataset as a few-shot source and conducted a 5-shot test on the test set.
  
- [MMLU](https://arxiv.org/abs/2009.03300) is an English foundation model evaluation benchmark that spans various domains like basic mathematics, American history, computer science, law, among others, with a total of 57 tasks. It evaluates language models' performance on different domain tasks. We performed a 5-shot test on this benchmark.
  
- [CMMLU](https://arxiv.org/abs/2306.09212) is a Chinese evaluation benchmark that encompasses 67 topics ranging from basic subjects to advanced professional levels. It evaluates Chinese language models' performance in knowledge and reasoning capabilities. We used the dev set of this dataset as a few-shot source and conducted a 5-shot test on the test set.
  
- [Gaokao](https://arxiv.org/abs/2305.12474) is an evaluation benchmark based on the dataset of Chinese college entrance examination questions. It aims to provide an assessment of Chinese language models in terms of language comprehension and logical reasoning capabilities. We retained only the four-option multiple-choice questions from it and conducted a 5-shot test after random partitioning.
  
- [HumanEval](https://arxiv.org/abs/2107.03374) is an English evaluation benchmark containing over one hundred coding problems. It assesses language models' abilities in code comprehension and generation. We adopted a zero-shot setting and the Pass@1 metric for testing the model.
  
- [GSM8K](https://arxiv.org/abs/2110.14168) is a dataset composed of high-quality elementary school math application problems. It requires the models to select the most appropriate solution based on the provided scenario and evaluates the models' capabilities in mathematical application. We conducted an 8-shot test on this benchmark.
  
- [BBH](https://arxiv.org/abs/2210.09261) is an evaluation benchmark formed from a selection of challenging tasks out of 204 Big-Bench benchmark tasks. We performed a 3-shot test on this benchmark.
  
- [AGIEval](https://arxiv.org/abs/2304.06364) is a benchmark to examine foundation models' human-like capabilities, specifically assessing foundational models' abilities in human cognition and problem-solving tasks. We retained only the four-option multiple-choice questions from it and conducted a 5-shot test after random partitioning.
  
These evaluation benchmarks provide standardized tests and metrics to assess language models' performance and capabilities across various tasks and domains. The evaluation results are shown in the table below:

| **Model**              |**C-Eval**| **MMLU** |**CMMLU** |**GAOKAO**|**HumanEval**|**GSM8K** | **BBH**  |**AGIEval**|
|:-----------------------|:--------:|:--------:|:--------:|:--------:|:-----------:|:--------:|:--------:|:---------:|
|                        |  5-shot  |  5-shot  |  5-shot  |  5-shot  |   0-shot    |  8-shot  |  3-shot  |  5-shot   |
| **GPT-4**              |   68.4   |   83.9   |   70.3   |   66.2   |    69.5     |   90.0   |   75.1   |   63.3    |
| **GPT-3.5 Turbo**      |   51.1   |   68.5   |   54.1   |   47.1   |    52.4     |   57.8   |   61.6   |   46.1    |
| **LLaMA2-7B**          |   28.9   |   45.7   |   31.4   |   26.0   |    12.8     |   16.2   |   39.2   |   26.5    |
| **ChatGLM2-6B**        |   50.2   |   45.9   |   49.0   |   49.4   |    9.2      |   28.9   |   31.7   |   45.3    |
| **Baichuan2-7B-Base**  |   54.0   |   54.2   |   57.1   |   47.5   |    18.3     |   24.5   |   41.6   |   42.7    |
| **Qwen-7B v1.1**       |   63.5   |   58.2   |   62.2   |     -    |    29.9     |   51.7   |   45.0   |     -     |
| **LingoWhale-8B-base** |   63.6   |   60.2   |   62.8   |   50.3   |    32.9     |   55.0   |   47.5   |   43.8    |

<span style="color:gray">For all of the above comparison models, we list their official reports and round them to one decimal place.</span>

# Generated Examples

The following examples are generated by a Chat model fine-tuned based on the LingoWhale-8B model using open source SFT data.

<details><summary><b>Mathematical calculations</b></summary><p>

**Query**ï¼šf(x)ä¸ºå¶å‡½æ•°ï¼Œ f(1) = 3, f(2) = 4, é‚£ä¹ˆf(1) + f(-2) ^2 ç­‰äºå¤šå°‘ï¼Ÿ è¯·ä¸€æ­¥ä¸€æ­¥è¯´æ˜

**Answer**ï¼šé¦–å…ˆï¼Œæ ¹æ®é¢˜ç›®ä¸­ç»™å‡ºçš„æ¡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼š

- f(x) æ˜¯ä¸€ä¸ªå¶å‡½æ•°ï¼Œå³ f(-x) = f(x)ã€‚
- f(1) = 3ã€‚
- f(2) = 4ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡º f(1) + f(-2)^2 çš„å€¼ã€‚
ç”±äº f(x) æ˜¯å¶å‡½æ•°ï¼Œæ‰€ä»¥ f(-2) = f(2)ï¼Œå› æ­¤ f(-2) = 4ã€‚
å°† f(1) å’Œ f(-2) çš„å€¼ä»£å…¥ f(1) + f(-2)^2 ä¸­ï¼Œå¾—åˆ°ï¼š
f(1) + f(-2)^2 = 3 + (4)^2 = 3 + 16 = 19ã€‚
å› æ­¤ï¼Œf(1) + f(-2)^2 çš„å€¼ä¸º 19ã€‚

</details>

<details><summary><b>Role playing</b></summary><p>

**Query**ï¼šä½œä¸ºæœ‰ä¸°å¯Œè¥é”€ç»éªŒçš„å“ç‰Œç»ç†ï¼Œè¯·ä¸ºæˆ‘æä¾›ä¸€æ¡å…³äºä»¥ä¸‹æƒ…å¢ƒçš„å“ç‰Œè¥é”€ç­–ç•¥ï¼š
æƒ…å¢ƒï¼š618æœŸé—´ï¼Œæé«˜å“ç‰Œé”€å”®é¢
å“ç‰Œå®šä½ï¼šå›½äº§ç²¾å“çœ‰ç¬”
å»ºè®®è¦æ±‚ï¼šæä¾›å…·æœ‰åˆ›æ–°æ€§å’Œå®æ•ˆæ€§çš„å“ç‰Œè¥é”€ç­–ç•¥ï¼Œå¸®åŠ©æå‡å“ç‰Œçš„é”€å”®é¢ï¼Œ å¹¶åŠ å…¥ç›´æ’­å¸¦è´§çš„æ–¹æ¡ˆã€‚

**Answer**ï¼šåœ¨618æœŸé—´ï¼Œä¸ºäº†æé«˜å“ç‰Œé”€å”®é¢ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–ä»¥ä¸‹å“ç‰Œè¥é”€ç­–ç•¥ï¼š

1. æ¨å‡ºé™æ—¶ä¼˜æƒ æ´»åŠ¨ï¼šåœ¨618æ´»åŠ¨æœŸé—´ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å“ç‰Œå®˜æ–¹ç½‘ç«™ä¸Šæ¨å‡ºé™æ—¶ä¼˜æƒ æ´»åŠ¨ï¼Œä¾‹å¦‚è´­ä¹°çœ‰ç¬”é€èµ å“ã€æ‰“æŠ˜ä¿ƒé”€ç­‰ï¼Œä»¥å¸å¼•æ¶ˆè´¹è€…çš„æ³¨æ„åŠ›å¹¶ä¿ƒè¿›é”€å”®ã€‚

2. åˆ©ç”¨ç¤¾äº¤åª’ä½“å®£ä¼ ï¼šæˆ‘ä»¬å¯ä»¥é€šè¿‡ç¤¾äº¤åª’ä½“å¹³å°ï¼ˆå¦‚å¾®åšã€å¾®ä¿¡ã€æŠ–éŸ³ç­‰ï¼‰å‘å¸ƒå“ç‰Œå®£ä¼ å†…å®¹ï¼ŒåŒ…æ‹¬äº§å“ä»‹ç»ã€ä½¿ç”¨æŠ€å·§ã€ç”¨æˆ·è¯„ä»·ç­‰ï¼Œä»¥å¢åŠ å“ç‰Œæ›å…‰ç‡å’Œç”¨æˆ·äº’åŠ¨æ€§ã€‚

3. ä¸çŸ¥ååšä¸»åˆä½œæ¨å¹¿ï¼šæˆ‘ä»¬å¯ä»¥é‚€è¯·ä¸€äº›çŸ¥åçš„ç¾å¦†åšä¸»æˆ–ç½‘çº¢æ¥è¯•ç”¨æˆ‘ä»¬çš„äº§å“ï¼Œå¹¶åœ¨å…¶ç¤¾äº¤åª’ä½“ä¸Šå‘å¸ƒæ¨èè§†é¢‘æˆ–å›¾æ–‡ï¼Œä»¥å¸å¼•æ›´å¤šçš„ç›®æ ‡å—ä¼—ã€‚

4. ç›´æ’­å¸¦è´§ï¼šæˆ‘ä»¬å¯ä»¥é‚€è¯·ä¸€äº›çŸ¥åçš„å¸¦è´§ä¸»æ’­æˆ–æ˜æ˜Ÿæ¥ä¸ºæˆ‘ä»¬çš„äº§å“è¿›è¡Œç›´æ’­å¸¦è´§ï¼Œé€šè¿‡ä»–ä»¬çš„å½±å“åŠ›æ¥å¸å¼•æ›´å¤šçš„æ¶ˆè´¹è€…å…³æ³¨å’Œè´­ä¹°ã€‚

5. å‚ä¸ç”µå•†å¹³å°æ´»åŠ¨ï¼šæˆ‘ä»¬å¯ä»¥ç§¯æå‚ä¸å„å¤§ç”µå•†å¹³å°çš„ä¿ƒé”€æ´»åŠ¨ï¼Œä¾‹å¦‚æ»¡å‡ã€æ»¡èµ ç­‰ï¼Œä»¥å¸å¼•æ›´å¤šçš„æ¶ˆè´¹è€…è´­ä¹°æˆ‘ä»¬çš„äº§å“ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œä»¥ä¸Šå“ç‰Œè¥é”€ç­–ç•¥å¯ä»¥å¸®åŠ©æˆ‘ä»¬åœ¨618æœŸé—´æé«˜å“ç‰Œé”€å”®é¢ï¼ŒåŒæ—¶åŠ å…¥ç›´æ’­å¸¦è´§çš„æ–¹æ¡ˆå¯ä»¥æ›´å¥½åœ°å¸å¼•ç›®æ ‡å—ä¼—ï¼Œæé«˜é”€å”®æ•ˆæœã€‚

</details>

<details><summary><b>Poetry Writing</b></summary><p>

**Query**ï¼šå†™ä¸€ä¸ªä¸ƒè¨€è¯—ï¼Œæè¿°å²³é£çš„è¯—ã€‚è¯—è¯å¸Œæœ›æ˜¯å®‹ä»£è¯—çš„é£æ ¼ã€‚

**Answer**ï¼šã€Šå²³é£ã€‹

é“é©¬é‡‘æˆˆå£®å£«é­‚ï¼Œ

ç²¾å¿ æŠ¥å›½å¿—å‡Œäº‘ã€‚

åƒå¤è‹±é›„è°å¯åŠï¼Œ

å²³å®¶å†›å¨éœ‡ä¹¾å¤ã€‚

</details>

<details><summary><b>Cultural knowledge</b></summary><p>

**Query**ï¼šæˆ‘è¦å»æˆéƒ½æ­¦ä¾¯ç¥ å‚è§‚ï¼Œèƒ½å¸®æˆ‘è¯¦ç»†ä»‹ç»ä¸‹æ­¦ä¾¯çš„ç”Ÿå¹³äº‹è¿¹å—ï¼Ÿ

**Answer**ï¼šè¯¸è‘›äº®ï¼ˆ181å¹´-234å¹´ï¼‰ï¼Œå­—å­”æ˜ï¼Œå·å§é¾™ï¼Œå¾å·ç…çŠé˜³éƒ½ï¼ˆä»Šå±±ä¸œä¸´æ²‚å¸‚æ²‚å—å¿ï¼‰äººï¼Œä¸‰å›½æ—¶æœŸèœ€æ±‰ä¸ç›¸ï¼Œæ°å‡ºçš„æ”¿æ²»å®¶ã€å†›äº‹å®¶ã€æ–‡å­¦å®¶ã€ä¹¦æ³•å®¶ã€å‘æ˜å®¶ã€‚æ—©å¹´éšå”çˆ¶è¯¸è‘›ç„åˆ°è†å·ï¼Œè¯¸è‘›ç„æ­»åï¼Œè¯¸è‘›äº®å°±åœ¨éš†ä¸­éšå±…ã€‚åˆ˜å¤‡ä¸‰é¡¾èŒ…åºè¯·å‡ºè¯¸è‘›äº®ï¼Œè”åˆä¸œå´å­™æƒäºèµ¤å£ä¹‹æˆ˜å¤§è´¥æ›¹å†›ï¼Œå½¢æˆä¸‰å›½é¼è¶³ä¹‹åŠ¿ï¼Œåˆå¤ºå è†ç›Šã€‚åˆ˜å¤‡å…¥èœ€åï¼Œè¯¸è‘›äº®é•‡å®ˆè†å·ã€‚åˆ˜å¤‡ç§°å¸åï¼Œè¯¸è‘›äº®è¢«ä»»å‘½ä¸ºä¸ç›¸ï¼Œä¸»æŒæœæ”¿ã€‚åä¸»åˆ˜ç¦…ç»§ä½ï¼Œè¯¸è‘›äº®è¢«å°ä¸ºæ­¦ä¹¡ä¾¯ï¼Œé¢†ç›Šå·ç‰§ã€‚å‹¤å‹‰è°¨æ…ï¼Œå¤§å°æ”¿äº‹å¿…äº²è‡ªå¤„ç†ï¼Œèµç½šä¸¥æ˜ï¼›ä¸ä¸œå´è”ç›Ÿï¼Œæ”¹å–„å’Œè¥¿å—å„æ—çš„å…³ç³»ï¼›å®è¡Œå±¯ç”°æ”¿ç­–ï¼ŒåŠ å¼ºæˆ˜å¤‡ã€‚å‰åäº”æ¬¡åŒ—ä¼ä¸­åŸï¼Œå¤šä»¥ç²®å°½æ— åŠŸã€‚ç»ˆå› ç§¯åŠ³æˆç–¾ï¼Œäº234å¹´ç—…é€äºäº”ä¸ˆåŸï¼ˆä»Šé™•è¥¿å²å±±å¿å—ï¼‰ï¼Œäº«å¹´54å²ã€‚åˆ˜ç¦…è¿½å°å…¶ä¸ºå¿ æ­¦ä¾¯ï¼Œåä¸–å¸¸ä»¥æ­¦ä¾¯å°Šç§°è¯¸è‘›äº®ã€‚ä¸œæ™‹æ”¿æƒå› å…¶å†›äº‹æ‰èƒ½ç‰¹è¿½å°ä»–ä¸ºæ­¦å…´ç‹ã€‚è¯¸è‘›äº®æ•£æ–‡ä»£è¡¨ä½œæœ‰ã€Šå‡ºå¸ˆè¡¨ã€‹ã€ã€Šè¯«å­ä¹¦ã€‹ç­‰ã€‚æ›¾å‘æ˜æœ¨ç‰›æµé©¬ã€å­”æ˜ç¯ç­‰ï¼Œå¹¶æ”¹é€ è¿å¼©ï¼Œå«åšè¯¸è‘›è¿å¼©ï¼Œå¯ä¸€å¼©åçŸ¢ä¿±å‘ã€‚è¯¸è‘›äº®ä¸€ç”Ÿâ€œé èº¬å°½ç˜ã€æ­»è€Œåå·²â€ï¼Œæ˜¯ä¸­å›½ä¼ ç»Ÿæ–‡åŒ–ä¸­å¿ è‡£ä¸æ™ºè€…çš„ä»£è¡¨äººç‰©ã€‚

</details>

<details><summary><b>Code generation</b></summary><p>

**Query**ï¼šä½ æ˜¯ä¸€åç¼–ç¨‹å¼€å‘è€…ï¼Œèƒ½å†™ä¸€ä¸ªæ­£åˆ™å¸®å¿™åˆ¤æ–­ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦æ˜¯åˆç†çš„æ‰‹æœºå·ç ï¼Œ ç”¨Pythonå®ç°ã€‚

**Answer**ï¼š
```python
import re

def is_valid_phone(phone):
    pattern = r'^1[3-9]\d{9}$'
    return bool(re.match(pattern, phone))

print(is_valid_phone('13800138000'))  # True
print(is_valid_phone('12345678901'))  # False
```

</details>

# Deployment and Inference

## Install Dependencies
```shell
pip install -r requirements.txt
```

+ To use flash attention, you need to install [flash-attn](https://github.com/Dao-AILab/flash-attention)

+ For using LoRA or other parameter efficient fine-tuning methods, please install [peft](https://github.com/huggingface/peft)

## Python Example

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("deeplang-ai/LingoWhale-8B", trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("deeplang-ai/LingoWhale-8B", device_map="auto", trust_remote_code=True)
>>> inputs = tokenizer("é™‹å®¤é“­\nå” åˆ˜ç¦¹é”¡\n", return_tensors="pt")
>>> inputs = inputs.to("cuda:0")
>>> pred = model.generate(**inputs, max_new_tokens=100, repetition_penalty=1.1)
>>> print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
```

# Fine-tuning
The fine-tuning example is based on `transformers.Trainer`. For a more deteiled guide on the arguments usage, please refer to Huggingface [`Trainer`](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#trainer) tutorial.

The aim of this section is to showcase the fine-tuning process. No guarantees are made about the model performance under this fine-tuning configuration.

## Single-Machine Training
Below is an example of fine-tuning on a single machine. The data used is 10,000 randomly selected instruction fine-tuning data from the COIG dataset. You can replace it with your own data.

```shell
hostfile=""
deepspeed --hostfile=$hostfile finetune/finetune.py  \
    --report_to "none" \
    --data_path "finetune/data/coig_10k.json" \
    --model_name_or_path deeplang-ai/LingoWhale-8B \
    --output_dir "output" \
    --model_max_length 2048 \
    --num_train_epochs 4 \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 1 \
    --save_strategy epoch \
    --learning_rate 2e-5 \
    --lr_scheduler_type constant \
    --adam_beta1 0.9 \
    --adam_beta2 0.98 \
    --adam_epsilon 1e-8 \
    --max_grad_norm 1.0 \
    --weight_decay 1e-4 \
    --warmup_ratio 0.0 \
    --logging_steps 1 \
    --gradient_checkpointing True \
    --deepspeed finetune/ds_config.json \
    --bf16 True \
    --tf32 True
```

To use you own data, please convert it to the json format below
```json
[
    {
        "id": 0,
        "conversations": [
            {
                "from": "human",
                "value": "è¯·é—®ä»€ä¹ˆæ˜¯â€œæ¨¡å¼å¹´é¾„â€ï¼Ÿ"
            },
            {
                "from": "model",
                "value": "æ¨¡å¼å¹´é¾„æ˜¯æŒ‡åˆ©ç”¨æ”¾å°„æ€§è¡°å˜è§„å¾‹å‡å®šåœ°è´¨æ ·å“å½¢æˆæ—¶çš„åˆå§‹åŒä½ç´ ç»„æˆè®¡ç®—å¾—åˆ°çš„å¹´é¾„ã€‚"
            },
            ...
        ]
    },
    ...
]
```

## Multi-Machine Training

For multi-machine training, you need to create a `hostfile` in the following format. Each line represents a machine. `ip_address-X` refers to the IP address of each machine, and the `slots` content indicates the number of available GPUs on the machine. The content format is as follows:

```
ip_address-1 slots=8
ip_address-2 slots=8
ip_address-3 slots=8
ip_address-4 slots=8
...
```

Next, specify the `hostfile` argument using the path of the `hostfile`, and run the following command to start multi-machine training.

```shell
hostfile="/path/to/hostfile"
deepspeed --hostfile=$hostfile finetune/finetune.py  \
    --report_to "none" \
    --data_path "finetune/data/coig_10k.json" \
    --model_name_or_path deeplang-ai/LingoWhale-8B \
    --output_dir "output" \
    --model_max_length 2048 \
    --num_train_epochs 4 \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 1 \
    --save_strategy epoch \
    --learning_rate 2e-5 \
    --lr_scheduler_type constant \
    --adam_beta1 0.9 \
    --adam_beta2 0.98 \
    --adam_epsilon 1e-8 \
    --max_grad_norm 1.0 \
    --weight_decay 1e-4 \
    --warmup_ratio 0.0 \
    --logging_steps 1 \
    --gradient_checkpointing True \
    --deepspeed finetune/ds_config.json \
    --bf16 True \
    --tf32 True
```

## Parameter-Efficient Fine-Tuning
By using [peft](https://github.com/huggingface/peft), you can easily apply parameter-efficient fine-tuning methods like LoRA, Prefix-Tuning, etc. The training method for LoRA is currently integrated into the code, which can be activated by adding `--use_lora True`.

LoRA checkpoints can be loaded using the following code:
```python
from peft import AutoPeftModelForCausalLM
model = AutoPeftModelForCausalLM.from_pretrained("output", trust_remote_code=True)
```

# Open Source License
The community use of the LingoWhale-8B model must adhere to the [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0) and the [LingoWhale-8B Model License Agreement](MODEL_LICENSE.md). If you wish to use this source model or its derivatives for commercial purposes, please refer to [LingoWhale-8B Model License Agreement](MODEL_LICENSE.md).
